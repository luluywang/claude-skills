# Critic Task

**Purpose:** Independent re-audit of the manuscript after fixer edits. Verifies fixes resolved the gaps and checks for new problems introduced.

---

## Input

- **Claims:** `current/claims.json` — the original claims
- **Previous audit:** `current/audit.json` (iteration 1) or prior critic output
- **Fixer output:** `current/fix_iterations/iteration_N_fixes.json` — what the fixer did
- **Referee soul documents:** `current/souls/{key}_soul.md` — narrative per-referee profiles for checking alignment (paths listed in `current/referee_profiles.json` under each referee's `soul_file`)
- **Referee index:** `current/referee_profiles.json` — lightweight index with `soul_file` paths and cross-referee analysis
- **Config:** `current/config.json` — manuscript and appendix file paths
- **Response document:** Path from config (for reference when re-checking claims)
- **Component references:**
  - `revisions/prompts/components/claim_taxonomy.prompt` — verification rules
  - `revisions/prompts/components/latex_conventions.prompt` — LaTeX conventions
  - `revisions/prompts/components/writing_quality.prompt` — **MANDATORY** writing standards for evaluating fixer prose
- **Iteration number:** Provided by orchestrator

## Task

Re-read the manuscript (with fixes applied) and independently verify every claim that was previously `missing` or `partial`. Also check that fixes didn't introduce new problems.

### Step 1: Re-Verify Previously Failing Claims

For each claim that was `missing` or `partial` in the previous audit/critic:

1. **Re-apply the same verification strategy** from `claim_taxonomy.prompt`
2. **Check if the fix resolved the issue:**
   - Search for keywords, labels, citations as before
   - If now found → `status: "resolved"`
   - If still missing → `status: "still_missing"`
   - If partially addressed → `status: "partial"`

3. **Record evidence** — quote the relevant line(s) from the manuscript

### Step 2: Verify Previously Passing Claims Still Pass

Spot-check a sample of claims that were `found` in the original audit to ensure fixer edits didn't break them:
- Check ~20% of previously-found claims (or all if fewer than 20 total)
- If any regressed → `status: "regression"` (a new problem)

### Step 3: Check for New Problems

Scan the fixer's edits for issues:

1. **Number hallucination (highest priority):** Check every number the fixer inserted. Does it appear in a table, figure, `\scalarinput{}`, or existing prose? If a number cannot be traced to a source in the manuscript, flag it as `hallucinated_number` — this is the most dangerous error and must be caught.
2. **Notation consistency:** Check every Greek letter, subscript, superscript, and parameter name the fixer used. Compare against existing definitions in the manuscript. Common failures:
   - Wrong subscript: `$\sigma$` instead of `$\sigma_\gamma$`
   - Wrong parameter name: "unobserved interchange fees" instead of "fee adjustment parameters"
   - Discrete sums where the model uses integrals (or vice versa)
   - Undefined LaTeX commands (e.g., `\1` instead of `\mathbf{1}`)
   Flag as `notation_error`.
3. **LLM tag correctness:** Every fixer edit should be inside `\begin{llm}...\end{llm}`. Flag any untagged edits.
4. **LaTeX syntax:** Check for unmatched braces, broken environments, missing `\end{}` tags.
5. **Reference integrity:** Any new `\ref{}` or `\textcite{}` added by the fixer must point to existing labels/keys.
6. **Writing quality (AI detection):** Read `writing_quality.prompt` and check every fixer edit against ALL rules (1-7). Flag violations as `writing_quality` issues. Common failures:
   - Meta-commentary: "proceeds as follows," "we now turn to," "this section discusses"
   - AI transitions: "Moreover," "Additionally," "Furthermore," "Notably"
   - AI vocabulary: align, crucial, delve, enhance, foster, leverage, robust, nuanced, comprehensive, facilitate, explicit (as intensifier)
   - Hedging stacks: multiple hedges in one sentence
   - Nominalizations: "make a calculation" instead of "calculate"
   - Latinate words where Anglo-Saxon works: utilize, facilitate, demonstrate, prior to
   - Template structures: "Not only... but also...", rule-of-three padding
   - Uniform sentence length (all ~20 words)
   - Passive voice where active is natural
   - **Verbosity: multi-paragraph formulation for a one-sentence point** (Rule 6)
   - **Artificial sequential structure: "First... Second... Third..." for joint procedures** (Rule 6)
   - **Missing takeaways: listing exercises without summarizing conclusions** (Rule 6)
   - **Lecturing tone: explaining TO the reader instead of reporting results** (Rule 6)
   - **Approximate signs (`\approx`) when exact solutions exist** (Rule 6)
   - **Legalistic language: "a fortiori," "mutatis mutandis"** (Rule 6)
   Each writing quality violation is a bug — count it toward `issues_remaining`.
7. **Content coherence:** Beyond writing quality, does the edit fit logically in context? Flag insertions that contradict or repeat surrounding text (but don't rewrite — that's the fixer's job in the next iteration).
8. **Duplicate content:** Did the fixer add content that already existed nearby? Flag redundancies.
9. **Over-deletion:** Did the fixer remove notation, results, or definitions that are referenced later in the manuscript? Search for `\ref{}`, variable names, and equation labels that point to deleted content. Flag as `over_deletion`.
10. **Content placement:** Is the content in the right file? Data construction details should be in the data appendix, not the estimation appendix. Confidential or single-firm results should not be in the published paper. Flag misplaced content.
11. **Regression check:** Compare the fixer's edit against the surrounding text. Did the rewrite lose key economic insights that existed before? If the edit is worse than what it replaced (less precise, missing the punchline, losing the intuition), flag as `regression`.
12. **Referee alignment:** For each resolved claim, read the originating referee's soul document at `current/souls/{key}_soul.md` (extract referee key from `source_comment`, e.g., `ref1_2_claim_1` → `ref1`). Check if the fix matches the referee's expectations:
   - **Quick Reference table:** Check evidence expectations, tone preferences, concession calibration
   - **Narrative sections:** Scan "What Do They Care About Most?" and "How Would They Want to Be Treated?" for contextual guidance — does the fix align with the spirit of what this referee wants?
   - A one-sentence fix for a referee with `very_high` or `high` evidence expectations → flag as `partial` with note to add data references
   - Apologetic framing ("We apologize...", "We regret...") for a referee with low apology tolerance → flag
   - Dismissive tone for a referee with low defense tolerance → flag
   - Patterns matching any item in the referee's Red Flags list → flag
13. **Fabricated verification:** Check for claims like "I have verified X," "I have computed X to verify Y," or "Computing Y confirms Z" in response document edits where no actual computation occurred. If the verification cannot be traced to a table, figure, `\scalarinput{}`, or code output in the manuscript, flag as `fabricated_verification`.

## Output

### Write `current/fix_iterations/iteration_N_critic.json`

```json
{
  "iteration": 1,
  "timestamp": "ISO timestamp",
  "claims_rechecked": 13,
  "resolved": 7,
  "still_missing": 3,
  "partial": 1,
  "regressions": 0,
  "new_issues_count": 2,
  "results": [
    {
      "claim_id": "ed_1a_claim_1",
      "previous_status": "missing",
      "new_status": "resolved",
      "evidence": "Line 342: '\\begin{llm}The results are weaker once...'",
      "file": "paper.tex",
      "referee_alignment": {
        "aligned": true,
        "issue": null
      }
    },
    {
      "claim_id": "ref1_2_claim_1",
      "previous_status": "missing",
      "new_status": "still_missing",
      "evidence": null,
      "note": "Keywords not found near target label",
      "referee_alignment": {
        "aligned": false,
        "issue": "Fix is one sentence; referee evidence_expectations is 'high'. Suggest adding data reference."
      }
    }
  ],
  "new_issues": [
    {
      "type": "untagged_edit",
      "file": "paper.tex",
      "line": 456,
      "description": "Fixer edit not wrapped in \\begin{llm}...\\end{llm}"
    },
    {
      "type": "writing_quality",
      "file": "paper.tex",
      "line": 342,
      "description": "AI meta-commentary: 'The explicit microfoundation proceeds as follows.' Replace with direct statement."
    }
  ],
  "issues_remaining": 6
}
```

The `issues_remaining` count is the total of `still_missing` + `partial` + `new_issues` count. This number is passed to `fix_loop.sh next` by the orchestrator.

### Return to Orchestrator (minimal)

```json
{
  "status": "complete",
  "iteration": 1,
  "resolved": 7,
  "still_missing": 3,
  "partial": 1,
  "regressions": 0,
  "new_issues": 2,
  "issues_remaining": 6
}
```

Do NOT return the full results inline. The orchestrator reads `current/fix_iterations/iteration_N_critic.json`.

## Rules

1. **Independent verification.** Do NOT trust the fixer's self-report. Re-read the actual manuscript files.
2. **Same standards as audit.** Apply the exact same verification criteria. A claim is `resolved` only if it passes the same checks that flagged it as `missing`.
3. **Don't fix anything.** The critic's job is to report, not edit. If something is still wrong, describe it clearly so the next fixer iteration can address it.
4. **Count accurately.** The `issues_remaining` number drives the stopping condition. An incorrect count can terminate the loop prematurely or extend it unnecessarily.
5. **Flag regressions prominently.** If a previously-passing claim now fails, this is a critical issue — the fixer broke something.
6. **Be specific about what's still missing.** "Keywords not found" is not enough — say which keywords, near which label, so the fixer knows exactly what to fix next iteration.
7. **Check LLM tags.** Untagged edits are a process violation. The user needs tags to identify AI-written content.
