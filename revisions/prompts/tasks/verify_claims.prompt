# Verify Claims Task

**Purpose:** Cross-check a draft response's claims against the actual manuscript to catch errors before compilation.

---

## Input

- **Response:** `current/responses/{id}.md` — the draft response to verify
- **Manuscript path:** From `current/config.json`

## Task

Read the draft response and verify every factual claim it makes about the manuscript.

### What to Verify

1. **Section references:** Does `\ref{subsec:model-assumptions}` point to a real label? Does the section contain what the response claims?

2. **Table references:** Does `\ref{tab:summary-stats}` exist? Does the table contain the data mentioned?

3. **Figure references:** Does `\ref{fig:event-study}` exist? Does the figure show what's described?

4. **Content claims:** When the response says "I have added a discussion of X in Section Y," verify:
   - Section Y exists
   - Note if the content can't be confirmed (may be a planned addition)

5. **Citation references:** When the response uses `\textcite{Smith2020}` or `\parencite{Jones.Doe2018}`, check if the citation key appears in the .bib file or manuscript.

6. **Numerical claims:** When the response cites specific numbers ("F-stat drops from 23 to 4"), check if these appear in the manuscript.

### What NOT to Verify

- The quality or persuasiveness of the response (that's the user's job)
- Whether the response adequately addresses the referee's concern
- Grammar or style of the response

## Verification Process

For each claim in the response:

1. **Extract the claim** — what specifically does the response assert about the manuscript?
2. **Locate the evidence** — search the manuscript for the referenced section/table/figure/number
3. **Assess** — does the evidence support the claim?

### Assessment Categories

- `verified` — Found in manuscript, matches the claim
- `likely_new` — Not in current manuscript; response describes a planned addition. Not an error.
- `mismatch` — Found in manuscript but content differs from what response claims
- `not_found` — Referenced label/section/figure doesn't exist in manuscript
- `unchecked` — Cannot verify (e.g., claim about data not in .tex file)

## Output: Write to Files

Write **two files** for each response you verify. Do NOT return the full results inline — write them to disk to keep the orchestrator's context lightweight.

### 1. Structured results → `current/verification/{id}.json`

```json
{
  "comment_id": "{id}",
  "claims_checked": 5,
  "verified": true,
  "issues": [
    "ref label 'subsec:oa-merchant-sorting' not found in manuscript"
  ],
  "results": [
    {
      "claim": "Section III.B discusses the Durbin Amendment evidence",
      "reference": "\\ref{subsec:durbin-reduced-form}",
      "status": "verified",
      "note": "Section exists and discusses Durbin"
    },
    {
      "claim": "I have added Appendix Table A.5 with robustness checks",
      "reference": "\\ref{tab:robustness-debit}",
      "status": "likely_new",
      "note": "Label not found — appears to be a planned addition"
    },
    {
      "claim": "The F-statistic drops from 23 to 4 in services",
      "reference": null,
      "status": "unchecked",
      "note": "Numerical claim — cannot verify from .tex alone"
    }
  ]
}
```

### 2. Human-readable issues → `current/verification/{id}.md`

```markdown
# Verification: {id}
**Status:** ✓ verified | ⚠ issues found
**Claims checked:** 5

## Issues
- ref label `subsec:oa-merchant-sorting` not found in manuscript
- Table \ref{tab:robustness-debit} — likely new (planned addition)

## Claim Details
| # | Claim | Status | Note |
|---|-------|--------|------|
| 1 | Section III.B discusses Durbin | verified | Section exists |
| 2 | Added Appendix Table A.5 | likely_new | Label not found |
| 3 | F-stat drops from 23 to 4 | unchecked | Cannot verify from .tex |
```

If there are **no issues**, the Issues section should say "None."

### 3. Return to orchestrator (minimal)

Return **only** a brief summary — the orchestrator will read the files for details:

```json
{
  "comment_id": "{id}",
  "verified": true,
  "issue_count": 0
}
```

## Rules

1. **Be strict about references.** A `\ref{}` to a non-existent label is a real problem — it will produce "??" in the compiled document.
2. **Be lenient about new content.** Many responses describe changes the user will make. Flag as `likely_new` rather than `not_found` when the response clearly indicates an addition.
3. **Check .bib files too.** If the manuscript includes `\bibliography{}` or `\addbibresource{}`, try to verify citation keys in those files.
4. **Don't modify the response.** Your job is to report, not fix. The orchestrator will read your files and present issues to the user.
5. **Always write both files.** Even if everything verifies cleanly, write the .json and .md files so the compile phase can read them.
